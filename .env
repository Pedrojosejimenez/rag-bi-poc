# Backend de vectores: faiss o qdrant
RAG_BACKEND=faiss

# Modelo de embeddings (gratuito)
EMBEDDINGS_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Generador LLM local vía Ollama; fallback extractor si no disponible
OLLAMA_BASE_URL=http://127.0.0.1:11434
OLLAMA_MODEL=llama3

# Modo de generación: ollama o stub
GENERATOR_MODE=ollama

# Ruta de datos
DATA_DIR=./data
PROCESSED_DIR=./data/processed

# Tamaño de chunk aproximado (en "tokens" aproximados por palabras)
CHUNK_TARGET_TOKENS=700
CHUNK_OVERLAP_TOKENS=120
